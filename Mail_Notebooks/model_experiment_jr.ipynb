{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, DateType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/26 14:20:51 WARN Utils: Your hostname, Julies-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.77 instead (on interface en0)\n",
      "24/06/26 14:20:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/26 14:20:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "scSpark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('data-consolidation.ipynb') \\\n",
    "    .config('spark.some.config.option', 'some-value') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scSpark.sparkContext.setLogLevel('OFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pull Destinating Pieces</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingPieces = scSpark.read.csv('../Data/Scan Data/Mail/Destinating Pieces pt. 1.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DestinatingPieces2 = scSpark.read.csv('../Data/Scan Data/Mail/Destinating Pieces pt. 1.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destinatingPieces = destinatingPieces1.union(DestinatingPieces2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Destinating Pieces files contain 44,210,232 records total. \n",
    "- 44,209,256 distinct unique indentifiers\n",
    "- 115 distinct stat the clock dates\n",
    "- 1587 distinct origin facilities\n",
    "- 12 distinct actual delivery dates\n",
    "- 107 distinct expected delivery dates\n",
    "- 3 distinct expected destination facilities: 'MEMPHIS - 1441274', 'MUSIC CITY ANNEX - 1532174', 'NASHVILLE - 1441275'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 3 distinct mail shape: 'Flat', 'Card', 'Letter'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingPieces_cleaned = destinatingPieces.dropna() \\\n",
    "                            .filter((destinatingPieces.START_THE_CLOCK_DATE != 'null') & \n",
    "                                (destinatingPieces.EXPECTED_DELIVERY_DATE != 'null') &\n",
    "                                (destinatingPieces.START_THE_CLOCK_DATE <= destinatingPieces.ACTUAL_DLVRY_DATE) &\n",
    "                                (destinatingPieces.EXPECTED_DESTINATION_FACILITY == 'MUSIC CITY ANNEX - 1532174') &\n",
    "                                (destinatingPieces.START_THE_CLOCK_DATE > '2023-12-22')) \\\n",
    "                            .selectExpr(\n",
    "                                '*',\n",
    "                                'count(*) over (partition by UNIQUE_IDENTIFIER) as cnt').filter(F.col('cnt') == 1).drop('cnt') \\\n",
    "                            .withColumn('daysDelivered', F.datediff(col('ACTUAL_DLVRY_DATE'), col('START_THE_CLOCK_DATE')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing nulls, the Destintaing Pieces files contains 39,683,888 records.\n",
    "- 39,682,919 distinct unique indentifiers\n",
    "- 114 distinct start the clock dates, 1327 distinct origin facilities\n",
    "- 11 distinct actual delivery dates, 106 distinct expected delivery dates\n",
    "- 3 distinct expected destination facilities: 'MEMPHIS - 1441274', 'MUSIC CITY ANNEX - 1532174', 'NASHVILLE - 1441275'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'3 distinct mail shape: 'Flat', 'Card', 'Letter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing records where the Start Time is after the Actual Delivery Date, the Destintaing Pieces files contains 39,338,233 records.\n",
    "- 39,337,266 distinct unique indentifiers\n",
    "- 99 distinct start the clock dates\n",
    "- 1324 distinct origin facilities\n",
    "- 11 distinct actual delivery dates\n",
    "- 100 distinct expected delivery dates\n",
    "- 3 distinct expected destination facilities: 'MEMPHIS - 1441274', 'MUSIC CITY ANNEX - 1532174', 'NASHVILLE - 1441275'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 3 distinct mail shape: 'Flat', 'Card', 'Letter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering the dataset for those expected to be delivered to Music City, the Destintaing Pieces files contains 3,100,202 records.\n",
    "- 3,100,107 distinct unique indentifiers\n",
    "- 89 distinct start the clock dates\n",
    "- 737 distinct origin facilities\n",
    "- 11 distinct actual delivery dates\n",
    "- 88 distinct expected delivery dates\n",
    "- 1 distinct expected destination facilities: 'MUSIC CITY ANNEX - 1532174'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 1 distinct mail shape: 'Flat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing peices that had a start date before 12/22/23, the Destintaing Pieces files contains 3,092,463 records.\n",
    "- 3,092,369 distinct unique indentifiers\n",
    "- 26 distinct start the clock dates\n",
    "- 709 distinct origin facilities\n",
    "- 11 distinct actual delivery dates\n",
    "- 28 distinct expected delivery dates\n",
    "- 1 distinct expected destination facilities: 'MUSIC CITY ANNEX - 1532174'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 1 distinct mail shape: 'Flat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing duplicates, the Destintaing Pieces files contains 3,092,317 records.\n",
    "- 3,092,317 distinct unique indentifiers\n",
    "- 26 distinct start the clock dates\n",
    "- 694 distinct origin facilities\n",
    "- 11 distinct actual delivery dates\n",
    "- 28 distinct expected delivery dates\n",
    "- 1 distinct expected destination facilities: 'MUSIC CITY ANNEX - 1532174'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 1 distinct mail shape: 'Flat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pull/Filter/Union Destinating Scans</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingScans = scSpark.read.csv('../Data/Scan Data/Mail/Destinating Scans pt. 1.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DestinatingScans2 = scSpark.read.csv('Scan Data/Mail/Destinating Scans pt. 2.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DestinatingScans3 = scSpark.read.csv('Scan Data/Mail/Destinating Scans pt. 3.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destinatingScans = DestinatingScans1.union(DestinatingScans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destinatingScans = destinatingScans.union(DestinatingScans3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Destinating Scans files contain 139,005,571 records total. \n",
    "- 53,730,012 distinct unique identifiers\n",
    "- 2,064,204 distinct scan_datetime\n",
    "- 370 distinct scan facilities\n",
    "- 229 distinct ops codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 'null' values\n",
    "destinatingScans_cleaned = destinatingScans.filter((destinatingScans.scan_datetime != 'null') & \n",
    "                                                   (destinatingScans.scan_facility != 'null') &\n",
    "                                                   (destinatingScans.op_code != 'null'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing null values, the Destinating Scans files contain 138,722,816 records total. \n",
    "- 53,621,334 distinct unique identifiers\n",
    "- 369 distinct scan facilities\n",
    "- 228 distinct ops codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsCodes = pd.read_excel('../teamkangaroo/usps_opscodes.xlsx', header = 1, sheet_name='OpCodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsCodes = opsCodes['Operation Code'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedCodes = [x for x in opsCodes if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedCodes = [int(x) for x in cleanedCodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scansGood = destinatingScans_cleaned.filter(destinatingScans_cleaned.op_code.isin(cleanedCodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scansGood = scansGood.groupBy('UNIQUE_IDENTIFIER').agg(F.count('op_code').alias('goodScanCnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "badScans = destinatingScans_cleaned.filter(~destinatingScans_cleaned.op_code.isin(cleanedCodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 399:=================================================>     (18 + 2) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|op_code| count|\n",
      "+-------+------+\n",
      "|      7|   524|\n",
      "|    232|  7133|\n",
      "|    987|   134|\n",
      "|    200|    72|\n",
      "|    101| 13131|\n",
      "|    854|  8872|\n",
      "|    961|   823|\n",
      "|    419|   129|\n",
      "|    964|     1|\n",
      "|    320|  1780|\n",
      "|    100|   956|\n",
      "|    326|   105|\n",
      "|    374|     5|\n",
      "|    130|   127|\n",
      "|    376|   147|\n",
      "|    271|     1|\n",
      "|    233| 14550|\n",
      "|    297|    54|\n",
      "|    858|  1856|\n",
      "|   -601|506775|\n",
      "+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "badScans.groupBy('op_code').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "badScans = badScans.groupBy('UNIQUE_IDENTIFIER').agg(F.count('op_code').alias('badScanCnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dScansIntervalTotal = destinatingScans_cleaned.groupBy('UNIQUE_IDENTIFIER') \\\n",
    "    .agg(F.min('scan_datetime').alias('minScan'), F.max('scan_datetime').alias('maxScan')) \\\n",
    "    .withColumn('scanIntervalTotal', F.datediff(col('maxScan'), col('minScan')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dScansMusicCity = destinatingScans_cleaned.filter(destinatingScans_cleaned.scan_facility == 'MUSIC CITY ANNEX - 1532174') \\\n",
    "    .groupBy('UNIQUE_IDENTIFIER') \\\n",
    "    .agg(F.min('scan_datetime').alias('minScanMC'), F.max('scan_datetime').alias('maxScanMC')) \\\n",
    "    .withColumn('scanIntervalMusicCity', F.datediff(col('maxScanMC'), col('minScanMC')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingMerge = destinatingPieces_cleaned.join(dScansIntervalTotal, ['UNIQUE_IDENTIFIER']) \\\n",
    "                                            .join(dScansMusicCity, ['UNIQUE_IDENTIFIER']) \\\n",
    "                                            .join(scansGood, ['UNIQUE_IDENTIFIER']) \\\n",
    "                                            .join(badScans, ['UNIQUE_IDENTIFIER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = scSpark.read.csv('../teamkangaroo/cleanedWeather.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingMerge = destinatingMerge.withColumn('mcScan1', F.to_date(col('minScanMC'))) \\\n",
    "    .withColumn('mcScan2', F.to_date(col('maxScanMC')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = destinatingMerge.join(weather, (destinatingMerge.mcScan1 == weather.start_date) & (destinatingMerge.mcScan2 == weather.end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df.withColumn('avgDAPR',final_df['avgDAPR'].cast(\"float\").alias('avgDAPR'))\n",
    "final_df=final_df.withColumn('avgMDPR',final_df['avgMDPR'].cast(\"float\").alias('avgMDPR'))\n",
    "final_df=final_df.withColumn('avgPRCP',final_df['avgPRCP'].cast(\"float\").alias('avgPRCP'))\n",
    "final_df=final_df.withColumn('avgSNOW',final_df['avgSNOW'].cast(\"float\").alias('avgSNOW'))\n",
    "final_df=final_df.withColumn('avgSNWD',final_df['avgSNWD'].cast(\"float\").alias('avgSNWD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.withColumn('isLate', F.when(final_df.ACTUAL_DLVRY_DATE > final_df.EXPECTED_DELIVERY_DATE, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_drop = ['START_THE_CLOCK_DATE', 'ACTUAL_DLVRY_DATE', 'EXPECTED_DELIVERY_DATE', \n",
    "                'EXPECTED_DESTINATION_FACILITY', 'MAIL_SHAPE', 'minScan', 'maxScan', \n",
    "                'minScanMC', 'maxScanMC', 'mcScan1', 'mcScan2', '_c0', 'start_date', 'end_date', 'UNIQUE_IDENTIFIER',\n",
    "                'ORIGIN_FACILITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(*columns_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAIL_CLASS',\n",
       " 'daysDelivered',\n",
       " 'scanIntervalTotal',\n",
       " 'scanIntervalMusicCity',\n",
       " 'goodScanCnt',\n",
       " 'badScanCnt',\n",
       " 'avgDAPR',\n",
       " 'avgMDPR',\n",
       " 'avgPRCP',\n",
       " 'avgSNOW',\n",
       " 'avgSNWD',\n",
       " 'isLate']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "367898"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:===================>                                      (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|isLate| count|\n",
      "+------+------+\n",
      "|     1| 15307|\n",
      "|     0|352591|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df.groupBy('isLate').count().show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:===============================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|daysDelivered| count|\n",
      "+-------------+------+\n",
      "|            0| 63521|\n",
      "|            1| 67099|\n",
      "|            2|166186|\n",
      "|            3| 43579|\n",
      "|            4| 11144|\n",
      "|            5|  6947|\n",
      "|            6|  1920|\n",
      "|            7|   967|\n",
      "|            8|   798|\n",
      "|            9|  1050|\n",
      "|           10|  1105|\n",
      "|           11|   888|\n",
      "|           12|   614|\n",
      "|           13|   828|\n",
      "|           14|   538|\n",
      "|           15|   259|\n",
      "|           16|   196|\n",
      "|           17|   154|\n",
      "|           18|    42|\n",
      "|           19|    31|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df.groupBy('daysDelivered').count().orderBy('daysDelivered').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:===================================================>     (9 + 1) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|scanIntervalTotal| count|\n",
      "+-----------------+------+\n",
      "|                0| 45017|\n",
      "|                1|278676|\n",
      "|                2| 23116|\n",
      "|                3|  9493|\n",
      "|                4|  2625|\n",
      "|                5|  1965|\n",
      "|                6|  1161|\n",
      "|                7|  1334|\n",
      "|                8|   950|\n",
      "|                9|   951|\n",
      "|               10|   477|\n",
      "|               11|   488|\n",
      "|               12|   507|\n",
      "|               13|   475|\n",
      "|               14|   268|\n",
      "|               15|   164|\n",
      "|               16|   131|\n",
      "|               17|    40|\n",
      "|               18|    21|\n",
      "|               19|    25|\n",
      "+-----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df.groupBy('scanIntervalTotal').count().orderBy('scanIntervalTotal').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['scanIntervalTotal', 'scanIntervalMusicCity', 'goodScanCnt', 'badScanCnt', 'avgDAPR', 'avgMDPR', \n",
    "               'avgPRCP', 'avgSNOW', 'avgSNWD'],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = assembler.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_df.select(\"features\", \"daysDelivered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"daysDelivered\", predictionCol=\"predicted_daysDelivered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"daysDelivered\", predictionCol=\"predicted_daysDelivered\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_r2 = RegressionEvaluator(labelCol=\"daysDelivered\", predictionCol=\"predicted_daysDelivered\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = evaluator_r2.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stages= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sindexer= StringIndexer(inputCols= ['MAIL_CLASS'], \n",
    "                        outputCols= [\"indexed_{}\".format(item) for item in ['MAIL_CLASS']],\n",
    "                        handleInvalid='keep',\n",
    "                        stringOrderType='frequencyDesc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['scanIntervalTotal', 'scanIntervalMusicCity', 'goodScanCnt', 'badScanCnt', 'avgDAPR', 'avgMDPR', \n",
    "               'avgPRCP', 'avgSNOW', 'avgSNWD', 'indexed_MAIL_CLASS'],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForestClassifier(labelCol=\"isLate\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[sindexer, assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_rf = pipeline.fit(final_df).transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_rf.randomSplit([0.7, 0.3], seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc= RandomForestClassifier(numTrees=70,\n",
    "                            maxDepth=3, \n",
    "                            labelCol='isLate',\n",
    "                            seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier_b0d973c963a7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 218:====================================================>  (19 + 1) / 20]\r"
     ]
    }
   ],
   "source": [
    "rfc_model= rfc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= rfc_model.transform(test_data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce= BinaryClassificationEvaluator(rawPredictionCol= \"rawPrediction\",\n",
    "                                   labelCol=\"isLate\", \n",
    "                                   metricName= \"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9764948743273518"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce.evaluate(preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAIL_CLASS',\n",
       " 'daysDelivered',\n",
       " 'scanIntervalTotal',\n",
       " 'scanIntervalMusicCity',\n",
       " 'goodScanCnt',\n",
       " 'badScanCnt',\n",
       " 'avgDAPR',\n",
       " 'avgMDPR',\n",
       " 'avgPRCP',\n",
       " 'avgSNOW',\n",
       " 'avgSNWD',\n",
       " 'isLate',\n",
       " 'indexed_MAIL_CLASS',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 286:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|       0.0|108149|\n",
      "|       1.0|  2732|\n",
      "+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds.groupBy('prediction').count().orderBy('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = rfc_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['scanIntervalTotal', 'scanIntervalMusicCity', 'goodScanCnt', 'badScanCnt', 'avgDAPR', 'avgMDPR', \n",
    "               'avgPRCP', 'avgSNOW', 'avgSNWD', 'indexed_MAIL_CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Imprtances:\n",
      "scanIntervalTotal: 0.6513\n",
      "scanIntervalMusicCity: 0.2081\n",
      "goodScanCnt: 0.0926\n",
      "badScanCnt: 0.0226\n",
      "avgDAPR: 0.0010\n",
      "avgMDPR: 0.0001\n",
      "avgPRCP: 0.0090\n",
      "avgSNOW: 0.0000\n",
      "avgSNWD: 0.0000\n",
      "indexed_MAIL_CLASS: 0.0154\n"
     ]
    }
   ],
   "source": [
    "print('Feature Imprtances:')\n",
    "\n",
    "for feature, importance in zip(feature_list, importance):\n",
    "    print(f'{feature}: {importance:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_data = final_df.drop(col('isLate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sindexer= StringIndexer(inputCols= ['MAIL_CLASS'], \n",
    "                        outputCols= [\"indexed_{}\".format(item) for item in ['MAIL_CLASS']],\n",
    "                        handleInvalid='keep',\n",
    "                        stringOrderType='frequencyDesc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['scanIntervalTotal', 'scanIntervalMusicCity', 'goodScanCnt', 'badScanCnt', 'avgDAPR', 'avgMDPR', \n",
    "               'avgPRCP', 'avgSNOW', 'avgSNWD', 'indexed_MAIL_CLASS'],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[sindexer, assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_rfr = pipeline.fit(final_df).transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_rf.randomSplit([0.7, 0.3], seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_reg = RandomForestRegressor(featuresCol=\"features\",labelCol=\"daysDelivered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = random_forest_reg.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 384:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data =  0.6504918086958457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"daysDelivered\"\\\n",
    "                                , predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "print (\"Root Mean Squared Error (RMSE) on test data = \",evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 397:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.8631681067393672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"daysDelivered\",\\\n",
    "                                predictionCol=\"prediction\", metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data =\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_rfr = model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['scanIntervalTotal', 'scanIntervalMusicCity', 'goodScanCnt', 'badScanCnt', 'avgDAPR', 'avgMDPR', \n",
    "               'avgPRCP', 'avgSNOW', 'avgSNWD', 'indexed_MAIL_CLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10, {0: 0.522, 1: 0.0468, 2: 0.0824, 3: 0.0325, 4: 0.0055, 5: 0.0013, 6: 0.174, 9: 0.1355})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
