{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, DateType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 11:23:08 WARN Utils: Your hostname, Julies-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.77 instead (on interface en0)\n",
      "24/06/23 11:23:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 11:23:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "scSpark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('data-consolidation.ipynb') \\\n",
    "    .config('spark.some.config.option', 'some-value') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scSpark.sparkContext.setLogLevel('OFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pull Destinating Pieces</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingPieces = scSpark.read.csv('Scan Data/Mail/Destinating Pieces pt. 1.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DestinatingPieces2 = scSpark.read.csv('Scan Data/Mail/Destinating Pieces pt. 2.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destinatingPieces = DestinatingPieces1.union(DestinatingPieces2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Destinating Pieces files contain 44,210,232 records total. \n",
    "- 44,209,256 distinct unique indentifiers\n",
    "- 115 distinct stat the clock dates\n",
    "- 1587 distinct origin facilities\n",
    "- 12 distinct actual delivery dates\n",
    "- 107 distinct expected delivery dates\n",
    "- 3 distinct expected destination facilities: 'MEMPHIS - 1441274', 'MUSIC CITY ANNEX - 1532174', 'NASHVILLE - 1441275'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 3 distinct mail shape: 'Flat', 'Card', 'Letter'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingPieces_cleaned = destinatingPieces.dropna() \\\n",
    "                            .filter((destinatingPieces.START_THE_CLOCK_DATE != 'null') & \n",
    "                                (destinatingPieces.EXPECTED_DELIVERY_DATE != 'null') &\n",
    "                                (destinatingPieces.START_THE_CLOCK_DATE <= destinatingPieces.ACTUAL_DLVRY_DATE) &\n",
    "                                (destinatingPieces.EXPECTED_DESTINATION_FACILITY == 'MUSIC CITY ANNEX - 1532174') &\n",
    "                                (destinatingPieces.START_THE_CLOCK_DATE > '2023-12-22')) \\\n",
    "                            .selectExpr(\n",
    "                                '*',\n",
    "                                'count(*) over (partition by UNIQUE_IDENTIFIER) as cnt').filter(F.col('cnt') == 1).drop('cnt') \\\n",
    "                            .withColumn('daysDelivered', F.datediff(col('ACTUAL_DLVRY_DATE'), col('START_THE_CLOCK_DATE')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing nulls, the Destintaing Pieces files contains 39,683,888 records.\n",
    "- 39,682,919 distinct unique indentifiers\n",
    "- 114 distinct start the clock dates, 1327 distinct origin facilities\n",
    "- 11 distinct actual delivery dates, 106 distinct expected delivery dates\n",
    "- 3 distinct expected destination facilities: 'MEMPHIS - 1441274', 'MUSIC CITY ANNEX - 1532174', 'NASHVILLE - 1441275'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'3 distinct mail shape: 'Flat', 'Card', 'Letter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing records where the Start Time is after the Actual Delivery Date, the Destintaing Pieces files contains 39,338,233 records.\n",
    "- 39,337,266 distinct unique indentifiers\n",
    "- 99 distinct start the clock dates\n",
    "- 1324 distinct origin facilities\n",
    "- 11 distinct actual delivery dates\n",
    "- 100 distinct expected delivery dates\n",
    "- 3 distinct expected destination facilities: 'MEMPHIS - 1441274', 'MUSIC CITY ANNEX - 1532174', 'NASHVILLE - 1441275'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 3 distinct mail shape: 'Flat', 'Card', 'Letter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering the dataset for those expected to be delivered to Music City, the Destintaing Pieces files contains 3,100,202 records.\n",
    "- 3,100,107 distinct unique indentifiers\n",
    "- 89 distinct start the clock dates\n",
    "- 737 distinct origin facilities\n",
    "- 11 distinct actual delivery dates\n",
    "- 88 distinct expected delivery dates\n",
    "- 1 distinct expected destination facilities: 'MUSIC CITY ANNEX - 1532174'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 1 distinct mail shape: 'Flat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing peices that had a start date before 12/22/23, the Destintaing Pieces files contains 3,092,463 records.\n",
    "- 3,092,369 distinct unique indentifiers\n",
    "- 26 distinct start the clock dates\n",
    "- 709 distinct origin facilities\n",
    "- 11 distinct actual delivery dates\n",
    "- 28 distinct expected delivery dates\n",
    "- 1 distinct expected destination facilities: 'MUSIC CITY ANNEX - 1532174'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 1 distinct mail shape: 'Flat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing duplicates, the Destintaing Pieces files contains 3,092,317 records.\n",
    "- 3,092,317 distinct unique indentifiers\n",
    "- 26 distinct start the clock dates\n",
    "- 694 distinct origin facilities\n",
    "- 11 distinct actual delivery dates\n",
    "- 28 distinct expected delivery dates\n",
    "- 1 distinct expected destination facilities: 'MUSIC CITY ANNEX - 1532174'\n",
    "- 4 distinct mail classes: 'USPS Marketing Mail', 'First Class Presort', 'Periodicals', 'Single Piece First Class'\n",
    "- 1 distinct mail shape: 'Flat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Pull/Filter/Union Destinating Scans</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingScans = scSpark.read.csv('Scan Data/Mail/Destinating Scans pt. 1.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DestinatingScans2 = scSpark.read.csv('Scan Data/Mail/Destinating Scans pt. 2.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DestinatingScans3 = scSpark.read.csv('Scan Data/Mail/Destinating Scans pt. 3.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destinatingScans = DestinatingScans1.union(DestinatingScans2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#destinatingScans = destinatingScans.union(DestinatingScans3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Destinating Scans files contain 139,005,571 records total. \n",
    "- 53,730,012 distinct unique identifiers\n",
    "- 2,064,204 distinct scan_datetime\n",
    "- 370 distinct scan facilities\n",
    "- 229 distinct ops codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 'null' values\n",
    "destinatingScans_cleaned = destinatingScans.filter((destinatingScans.scan_datetime != 'null') & \n",
    "                                                   (destinatingScans.scan_facility != 'null') &\n",
    "                                                   (destinatingScans.op_code != 'null'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing null values, the Destinating Scans files contain 138,722,816 records total. \n",
    "- 53,621,334 distinct unique identifiers\n",
    "- 369 distinct scan facilities\n",
    "- 228 distinct ops codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsCodes = pd.read_excel('../teamkangaroo/usps_opscodes.xlsx', header = 1, sheet_name='OpCodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opsCodes = opsCodes['Operation Code'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedCodes = [x for x in opsCodes if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedCodes = [int(x) for x in cleanedCodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scansGood = destinatingScans_cleaned.filter(destinatingScans_cleaned.op_code.isin(cleanedCodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scansGood = scansGood.groupBy('UNIQUE_IDENTIFIER').agg(F.count('op_code').alias('goodScanCnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "badScans = destinatingScans_cleaned.filter(~destinatingScans_cleaned.op_code.isin(cleanedCodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "badScans = badScans.groupBy('UNIQUE_IDENTIFIER').agg(F.count('op_code').alias('badScanCnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dScansIntervalTotal = destinatingScans_cleaned.groupBy('UNIQUE_IDENTIFIER') \\\n",
    "    .agg(F.min('scan_datetime').alias('minScan'), F.max('scan_datetime').alias('maxScan')) \\\n",
    "    .withColumn('scanIntervalTotal', F.datediff(col('maxScan'), col('minScan')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dScansMusicCity = destinatingScans_cleaned.filter(destinatingScans_cleaned.scan_facility == 'MUSIC CITY ANNEX - 1532174') \\\n",
    "    .groupBy('UNIQUE_IDENTIFIER') \\\n",
    "    .agg(F.min('scan_datetime').alias('minScanMC'), F.max('scan_datetime').alias('maxScanMC')) \\\n",
    "    .withColumn('scanIntervalMusicCity', F.datediff(col('maxScanMC'), col('minScanMC')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingMerge = destinatingPieces_cleaned.join(dScansIntervalTotal, ['UNIQUE_IDENTIFIER']) \\\n",
    "                                            .join(dScansMusicCity, ['UNIQUE_IDENTIFIER']) \\\n",
    "                                            .join(scansGood, ['UNIQUE_IDENTIFIER']) \\\n",
    "                                            .join(badScans, ['UNIQUE_IDENTIFIER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = scSpark.read.csv('../teamkangaroo/cleanedWeather.csv', header = True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinatingMerge = destinatingMerge.withColumn('mcScan1', F.to_date(col('minScanMC'))) \\\n",
    "    .withColumn('mcScan2', F.to_date(col('maxScanMC')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = destinatingMerge.join(weather, (destinatingMerge.mcScan1 == weather.start_date) & (destinatingMerge.mcScan2 == weather.end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=final_df.withColumn('avgDAPR',final_df['avgDAPR'].cast(\"float\").alias('avgDAPR'))\n",
    "final_df=final_df.withColumn('avgMDPR',final_df['avgMDPR'].cast(\"float\").alias('avgMDPR'))\n",
    "final_df=final_df.withColumn('avgPRCP',final_df['avgPRCP'].cast(\"float\").alias('avgPRCP'))\n",
    "final_df=final_df.withColumn('avgSNOW',final_df['avgSNOW'].cast(\"float\").alias('avgSNOW'))\n",
    "final_df=final_df.withColumn('avgSNWD',final_df['avgSNWD'].cast(\"float\").alias('avgSNWD'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.withColumn('isLate', F.when(final_df.ACTUAL_DLVRY_DATE > final_df.EXPECTED_DELIVERY_DATE, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_drop = ['START_THE_CLOCK_DATE', 'ACTUAL_DLVRY_DATE', 'EXPECTED_DELIVERY_DATE', \n",
    "                'EXPECTED_DESTINATION_FACILITY', 'MAIL_SHAPE', 'minScan', 'maxScan', \n",
    "                'minScanMC', 'maxScanMC', 'mcScan1', 'mcScan2', '_c0', 'start_date', 'end_date', 'UNIQUE_IDENTIFIER',\n",
    "                'ORIGIN_FACILITY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop(*columns_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAIL_CLASS',\n",
       " 'daysDelivered',\n",
       " 'scanIntervalTotal',\n",
       " 'scanIntervalMusicCity',\n",
       " 'goodScanCnt',\n",
       " 'badScanCnt',\n",
       " 'avgDAPR',\n",
       " 'avgMDPR',\n",
       " 'avgPRCP',\n",
       " 'avgSNOW',\n",
       " 'avgSNWD',\n",
       " 'isLate']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['scanIntervalTotal', 'scanIntervalMusicCity', 'goodScanCnt', 'badScanCnt', 'avgDAPR', 'avgMDPR', \n",
    "               'avgPRCP', 'avgSNOW', 'avgSNWD'],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = assembler.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_df.select(\"features\", \"daysDelivered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"daysDelivered\", predictionCol=\"predicted_daysDelivered\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"daysDelivered\", predictionCol=\"predicted_daysDelivered\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_r2 = RegressionEvaluator(labelCol=\"daysDelivered\", predictionCol=\"predicted_daysDelivered\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = evaluator_r2.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StandardScaler, VectorAssembler, StringIndexer, Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_stages= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sindexer= StringIndexer(inputCols= ['MAIL_CLASS'], \n",
    "                        outputCols= [\"indexed_{}\".format(item) for item in ['MAIL_CLASS']],\n",
    "                        handleInvalid='keep',\n",
    "                        stringOrderType='frequencyDesc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['scanIntervalTotal', 'scanIntervalMusicCity', 'goodScanCnt', 'badScanCnt', 'avgDAPR', 'avgMDPR', \n",
    "               'avgPRCP', 'avgSNOW', 'avgSNWD', 'indexed_MAIL_CLASS'],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForestClassifier(labelCol=\"isLate\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[sindexer, assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_rf = pipeline.fit(final_df).transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_rf.randomSplit([0.7, 0.3], seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc= RandomForestClassifier(numTrees=70,\n",
    "                            maxDepth=3, \n",
    "                            labelCol='isLate',\n",
    "                            seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier_b58cad7c0223"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 220:====================================================>  (19 + 1) / 20]\r"
     ]
    }
   ],
   "source": [
    "rfc_model= rfc.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= rfc_model.transform(test_data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce= BinaryClassificationEvaluator(rawPredictionCol= \"rawPrediction\",\n",
    "                                   labelCol=\"isLate\", \n",
    "                                   metricName= \"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9764948743273518"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce.evaluate(preds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_data = final_df.drop(col('isLate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sindexer= StringIndexer(inputCols= ['MAIL_CLASS'], \n",
    "                        outputCols= [\"indexed_{}\".format(item) for item in ['MAIL_CLASS']],\n",
    "                        handleInvalid='keep',\n",
    "                        stringOrderType='frequencyDesc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['scanIntervalTotal', 'scanIntervalMusicCity', 'goodScanCnt', 'badScanCnt', 'avgDAPR', 'avgMDPR', \n",
    "               'avgPRCP', 'avgSNOW', 'avgSNWD', 'indexed_MAIL_CLASS'],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[sindexer, assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_rfr = pipeline.fit(final_df).transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_rf.randomSplit([0.7, 0.3], seed=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_reg = RandomForestRegressor(featuresCol=\"features\",labelCol=\"daysDelivered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = random_forest_reg.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 348:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data =  0.6701515000365502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"daysDelivered\"\\\n",
    "                                , predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "print (\"Root Mean Squared Error (RMSE) on test data = \",evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 361:============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.8547722329343596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"daysDelivered\",\\\n",
    "                                predictionCol=\"prediction\", metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data =\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
