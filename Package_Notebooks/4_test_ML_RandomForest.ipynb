{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d4b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9295/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ff9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26246b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/22 10:58:01 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.20 instead (on interface wlp61s0)\n",
      "24/06/22 10:58:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/22 10:58:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.functions import col, unix_timestamp, round\n",
    "from pyspark.sql.functions import col, datediff, hour, dayofweek\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"USPS_ML\") \\\n",
    "    .config(\"spark.executor.memory\", \"15G\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153826d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-------------+-------------+---------+----------+---------+-----------+-----------+-----------+-----------+-----------+----------+-----------+------------+----------+-----------+------------+\n",
      "|MailPieceID|ServiceTypeCode|Origin_zip|ScheduledDeliveryDate|destination_zip|MailClassCode|total_scans|Distinct_zip_scans|Distinct_event_scans|time_delta_minutes|Origin_date|destination_date|late|YEARMONTH_Ori|YEARMONTH_Des|    LAT_O|     LNG_O|    LAT_D|      LNG_D|LAT_O_ROUND|LNG_O_ROUND|LAT_D_ROUND|LNG_D_ROUND|EVENT_ID_O|YEARMONTH_O|EVENT_TYPE_O|EVENT_ID_D|YEARMONTH_D|EVENT_TYPE_D|\n",
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-------------+-------------+---------+----------+---------+-----------+-----------+-----------+-----------+-----------+----------+-----------+------------+----------+-----------+------------+\n",
      "|85422102810|           058 |     37212|           2024-01-10|          02458|           PM|         12|                 6|                  10|            174240| 2023-09-11|      2024-01-10|   0|       202309|       202401|36.133877|-86.801254|42.353585| -71.188192|       36.1|      -86.8|       42.4|      -71.2|      NULL|       NULL|        NULL|      NULL|       NULL|        NULL|\n",
      "|85536261968|           019 |     37167|           2024-01-11|          37228|           PM|         16|                 6|                  13|            167040| 2023-09-18|      2024-01-12|   1|       202309|       202401|35.959541| -86.53159| 36.19693| -86.803373|       36.0|      -86.5|       36.2|      -86.8|      NULL|       NULL|        NULL|      NULL|       NULL|        NULL|\n",
      "|85567880556|           019 |     37208|           2024-01-11|          79407|           PM|         12|                 6|                   9|            164160| 2023-09-20|      2024-01-12|   1|       202309|       202401|36.177714|-86.808023|33.566862|-102.079243|       36.2|      -86.8|       33.6|     -102.1|      NULL|       NULL|        NULL|      NULL|       NULL|        NULL|\n",
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-------------+-------------+---------+----------+---------+-----------+-----------+-----------+-----------+-----------+----------+-----------+------------+----------+-----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/home/pk/DAEN690/PKG_final.parquet\")\n",
    "#show 3 rows of our DataFrame\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48532220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-----------+-----------+-----------+-----------+----------+------------+----------+------------+\n",
      "|MailPieceID|ServiceTypeCode|Origin_zip|ScheduledDeliveryDate|destination_zip|MailClassCode|total_scans|Distinct_zip_scans|Distinct_event_scans|time_delta_minutes|Origin_date|destination_date|late|LAT_O_ROUND|LNG_O_ROUND|LAT_D_ROUND|LNG_D_ROUND|EVENT_ID_O|EVENT_TYPE_O|EVENT_ID_D|EVENT_TYPE_D|\n",
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-----------+-----------+-----------+-----------+----------+------------+----------+------------+\n",
      "|85422102810|           058 |     37212|           2024-01-10|          02458|           PM|         12|                 6|                  10|            174240| 2023-09-11|      2024-01-10|   0|       36.1|      -86.8|       42.4|      -71.2|      NULL|        NULL|      NULL|        NULL|\n",
      "|85536261968|           019 |     37167|           2024-01-11|          37228|           PM|         16|                 6|                  13|            167040| 2023-09-18|      2024-01-12|   1|       36.0|      -86.5|       36.2|      -86.8|      NULL|        NULL|      NULL|        NULL|\n",
      "|85567880556|           019 |     37208|           2024-01-11|          79407|           PM|         12|                 6|                   9|            164160| 2023-09-20|      2024-01-12|   1|       36.2|      -86.8|       33.6|     -102.1|      NULL|        NULL|      NULL|        NULL|\n",
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-----------+-----------+-----------+-----------+----------+------------+----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\"YEARMONTH_o\",\"YEARMONTH_Ori\",\"LAT_O\",\"LNG_O\",\"YEARMONTH_D\",\"YEARMONTH_Ori\",\"YEARMONTH_Des\",\"LAT_D\",\"LNG_D\"]\n",
    "\n",
    "df = df.drop(*columns_to_drop)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afaa6f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-----------+-----------+-----------+-----------+----------+------------+----------+------------+-----------------+----------------------+---------------------+---------------+--------------+------------------+------------+-----------+-----------------------+-----------------+----------------+\n",
      "|MailPieceID|ServiceTypeCode|Origin_zip|ScheduledDeliveryDate|destination_zip|MailClassCode|total_scans|Distinct_zip_scans|Distinct_event_scans|time_delta_minutes|Origin_date|destination_date|late|LAT_O_ROUND|LNG_O_ROUND|LAT_D_ROUND|LNG_D_ROUND|EVENT_ID_O|EVENT_TYPE_O|EVENT_ID_D|EVENT_TYPE_D|Origin_zip_bucket|destination_zip_bucket|day_of_week_Scheduled|month_Scheduled|year_Scheduled|day_of_week_Origin|month_Origin|year_Origin|day_of_week_Destination|month_Destination|year_Destination|\n",
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-----------+-----------+-----------+-----------+----------+------------+----------+------------+-----------------+----------------------+---------------------+---------------+--------------+------------------+------------+-----------+-----------------------+-----------------+----------------+\n",
      "|85422102810|           058 |     37212|           2024-01-10|           2458|           PM|         12|                 6|                  10|            174240| 2023-09-11|      2024-01-10|   0|       36.1|      -86.8|       42.4|      -71.2|      NULL|        NULL|      NULL|        NULL|                4|                     3|                    4|              1|          2024|                 2|           9|       2023|                      4|                1|            2024|\n",
      "|85536261968|           019 |     37167|           2024-01-11|          37228|           PM|         16|                 6|                  13|            167040| 2023-09-18|      2024-01-12|   1|       36.0|      -86.5|       36.2|      -86.8|      NULL|        NULL|      NULL|        NULL|                4|                     4|                    5|              1|          2024|                 2|           9|       2023|                      6|                1|            2024|\n",
      "|85567880556|           019 |     37208|           2024-01-11|          79407|           PM|         12|                 6|                   9|            164160| 2023-09-20|      2024-01-12|   1|       36.2|      -86.8|       33.6|     -102.1|      NULL|        NULL|      NULL|        NULL|                4|                     8|                    5|              1|          2024|                 4|           9|       2023|                      6|                1|            2024|\n",
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-----------+-----------+-----------+-----------+----------+------------+----------+------------+-----------------+----------------------+---------------------+---------------+--------------+------------------+------------+-----------+-----------------------+-----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, dayofweek, month, year, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn(\"Origin_zip\", col(\"Origin_zip\").cast(\"int\")) \\\n",
    "       .withColumn(\"destination_zip\", col(\"destination_zip\").cast(\"int\"))\n",
    "\n",
    "\n",
    "def categorize_zip_by_first_three_digits(zip_code):\n",
    "    first_three_digits = int(str(zip_code)[:3])  \n",
    "    if 0 <= first_three_digits <= 149:\n",
    "        return 1  # Northeast\n",
    "    elif 150 <= first_three_digits <= 199:\n",
    "        return 2  # Southeast\n",
    "    elif 200 <= first_three_digits <= 299:\n",
    "        return 3  # South Central\n",
    "    elif 300 <= first_three_digits <= 399:\n",
    "        return 4  # North Central\n",
    "    elif 400 <= first_three_digits <= 499:\n",
    "        return 5  # Great Lakes\n",
    "    elif 500 <= first_three_digits <= 599:\n",
    "        return 6  # Midwest\n",
    "    elif 600 <= first_three_digits <= 699:\n",
    "        return 7  # South\n",
    "    elif 700 <= first_three_digits <= 799:\n",
    "        return 8  # Mountain\n",
    "    elif 800 <= first_three_digits <= 899:\n",
    "        return 9  # Southwest\n",
    "    elif 900 <= first_three_digits <= 999:\n",
    "        return 10  # West\n",
    "    else:\n",
    "        return -1  # Invalid ZIP code\n",
    "\n",
    "\n",
    "categorize_udf = udf(categorize_zip_by_first_three_digits, IntegerType())\n",
    "\n",
    "\n",
    "df = df.withColumn(\"Origin_zip_bucket\", categorize_udf(col(\"Origin_zip\")))\n",
    "df = df.withColumn(\"destination_zip_bucket\", categorize_udf(col(\"destination_zip\")))\n",
    "\n",
    "df = df.withColumn(\"day_of_week_Scheduled\", dayofweek(\"ScheduledDeliveryDate\"))\n",
    "df = df.withColumn(\"month_Scheduled\", month(\"ScheduledDeliveryDate\"))\n",
    "df = df.withColumn(\"year_Scheduled\", year(\"ScheduledDeliveryDate\"))\n",
    "\n",
    "df = df.withColumn(\"day_of_week_Origin\", dayofweek(\"Origin_date\"))\n",
    "df = df.withColumn(\"month_Origin\", month(\"Origin_date\"))\n",
    "df = df.withColumn(\"year_Origin\", year(\"Origin_date\"))\n",
    "\n",
    "df = df.withColumn(\"day_of_week_Destination\", dayofweek(\"destination_date\"))\n",
    "df = df.withColumn(\"month_Destination\", month(\"destination_date\"))\n",
    "df = df.withColumn(\"year_Destination\", year(\"destination_date\"))\n",
    "\n",
    "# Write DataFrame to parquet file\n",
    "df.write.mode('overwrite').parquet(\"/home/pk/DAEN690/PKG_ML.parquet\")\n",
    "print(\"done\")\n",
    "df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa8df3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows:  1228246\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.count()\n",
    "print(\"number of rows: \", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e725a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MailPieceID: string (nullable = true)\n",
      " |-- ServiceTypeCode: string (nullable = true)\n",
      " |-- Origin_zip: integer (nullable = true)\n",
      " |-- ScheduledDeliveryDate: date (nullable = true)\n",
      " |-- destination_zip: integer (nullable = true)\n",
      " |-- MailClassCode: string (nullable = true)\n",
      " |-- total_scans: long (nullable = true)\n",
      " |-- Distinct_zip_scans: long (nullable = true)\n",
      " |-- Distinct_event_scans: long (nullable = true)\n",
      " |-- time_delta_minutes: integer (nullable = true)\n",
      " |-- Origin_date: date (nullable = true)\n",
      " |-- destination_date: date (nullable = true)\n",
      " |-- late: integer (nullable = true)\n",
      " |-- LAT_O_ROUND: double (nullable = true)\n",
      " |-- LNG_O_ROUND: double (nullable = true)\n",
      " |-- LAT_D_ROUND: double (nullable = true)\n",
      " |-- LNG_D_ROUND: double (nullable = true)\n",
      " |-- EVENT_ID_O: string (nullable = true)\n",
      " |-- EVENT_TYPE_O: string (nullable = true)\n",
      " |-- EVENT_ID_D: string (nullable = true)\n",
      " |-- EVENT_TYPE_D: string (nullable = true)\n",
      " |-- Origin_zip_bucket: integer (nullable = true)\n",
      " |-- destination_zip_bucket: integer (nullable = true)\n",
      " |-- day_of_week_Scheduled: integer (nullable = true)\n",
      " |-- month_Scheduled: integer (nullable = true)\n",
      " |-- year_Scheduled: integer (nullable = true)\n",
      " |-- day_of_week_Origin: integer (nullable = true)\n",
      " |-- month_Origin: integer (nullable = true)\n",
      " |-- year_Origin: integer (nullable = true)\n",
      " |-- day_of_week_Destination: integer (nullable = true)\n",
      " |-- month_Destination: integer (nullable = true)\n",
      " |-- year_Destination: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "619e1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ecbfcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a4b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354a628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"USPS_ML\") \\\n",
    "    .config(\"spark.executor.memory\", \"10G\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abf47603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-----------+-----------+-----------+-----------+----------+------------+----------+------------+-----------------+----------------------+---------------------+---------------+--------------+------------------+------------+-----------+-----------------------+-----------------+----------------+\n",
      "|MailPieceID|ServiceTypeCode|Origin_zip|ScheduledDeliveryDate|destination_zip|MailClassCode|total_scans|Distinct_zip_scans|Distinct_event_scans|time_delta_minutes|Origin_date|destination_date|late|LAT_O_ROUND|LNG_O_ROUND|LAT_D_ROUND|LNG_D_ROUND|EVENT_ID_O|EVENT_TYPE_O|EVENT_ID_D|EVENT_TYPE_D|Origin_zip_bucket|destination_zip_bucket|day_of_week_Scheduled|month_Scheduled|year_Scheduled|day_of_week_Origin|month_Origin|year_Origin|day_of_week_Destination|month_Destination|year_Destination|\n",
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-----------+-----------+-----------+-----------+----------+------------+----------+------------+-----------------+----------------------+---------------------+---------------+--------------+------------------+------------+-----------+-----------------------+-----------------+----------------+\n",
      "|85730794758|           490 |     37217|           2024-01-18|          15601|           BS|         31|                 7|                  14|            159840| 2023-09-27|      2024-01-16|   0|       36.1|      -86.7|       40.3|      -79.5|      NULL|        NULL|      NULL|        NULL|                4|                     2|                    5|              1|          2024|                 4|           9|       2023|                      3|                1|            2024|\n",
      "|85754705957|           490 |     37217|           2024-01-18|          92225|           BS|         31|                 8|                  14|            158400| 2023-09-28|      2024-01-16|   0|       36.1|      -86.7|       33.7|     -114.7|      NULL|        NULL|      NULL|        NULL|                4|                    10|                    5|              1|          2024|                 5|           9|       2023|                      3|                1|            2024|\n",
      "|85825424934|           490 |     37217|           2024-01-17|          92120|           BS|         30|                 6|                  15|            148320| 2023-10-02|      2024-01-13|   0|       36.1|      -86.7|       32.8|     -117.1|      NULL|        NULL|      NULL|        NULL|                4|                    10|                    4|              1|          2024|                 2|          10|       2023|                      7|                1|            2024|\n",
      "+-----------+---------------+----------+---------------------+---------------+-------------+-----------+------------------+--------------------+------------------+-----------+----------------+----+-----------+-----------+-----------+-----------+----------+------------+----------+------------+-----------------+----------------------+---------------------+---------------+--------------+------------------+------------+-----------+-----------------------+-----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp = spark.read.parquet(\"/home/pk/DAEN690/PKG_ML.parquet\")\n",
    "df = df_temp.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29346916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error % \n",
      "18.00705553387957\n"
     ]
    }
   ],
   "source": [
    "total_count = df.count()\n",
    "\n",
    "count_ones = df.filter(col(\"late\") == 1).count()\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_ones = (count_ones / total_count) * 100\n",
    "print(\"Error % \")\n",
    "print(percentage_ones )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae4790d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8258924907673368\n",
      "Best model hyperparameters:\n",
      "Num Trees: 10\n",
      "Max Depth: 15\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    df = df.drop('label')\n",
    "\n",
    "label_indexer = StringIndexer(inputCol=\"late\", outputCol=\"label\")\n",
    "df = label_indexer.fit(df).transform(df)\n",
    "\n",
    "selected_features = [ \"Origin_zip_bucket\", \"destination_zip_bucket\",\n",
    "                     \"ServiceTypeCode\", \"MailClassCode\", \"total_scans\", \"Distinct_zip_scans\",\n",
    "                     \"Distinct_event_scans\", \"time_delta_minutes\", \"LAT_O_ROUND\", \"LNG_O_ROUND\",\n",
    "                     \"LAT_D_ROUND\", \"LNG_D_ROUND\", \"EVENT_ID_O\", \"EVENT_ID_D\", \"EVENT_TYPE_O\",\n",
    "                     \"EVENT_TYPE_D\", \"label\"]\n",
    "\n",
    "\n",
    "data = df.select(selected_features)\n",
    "\n",
    "categorical_cols = [\"ServiceTypeCode\",  \"MailClassCode\", \"EVENT_ID_O\", \"EVENT_ID_D\"]\n",
    "\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\").fit(data)\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "# One-hot encode indexed categorical columns\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_onehot\")\n",
    "    for col in categorical_cols\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "data_encoded = pipeline.fit(data).transform(data)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=[col + \"_onehot\" for col in categorical_cols], outputCol=\"features\")\n",
    "data_final = assembler.transform(data_encoded).select(\"features\", \"label\")\n",
    "\n",
    "# Split data into training and test sets (80% train, 20% test)\n",
    "train_data, test_data = data_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define hyperparameters for RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "cv = CrossValidator(estimator=rf, estimatorParamMaps=param_grid, evaluator=MulticlassClassificationEvaluator(), numFolds=5)\n",
    "\n",
    "# Train the model using cross-validation\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "# Evaluate model performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Print best model hyperparameters\n",
    "best_model = cv_model.bestModel\n",
    "print(\"Best model hyperparameters:\")\n",
    "print(\"Num Trees:\", best_model.getNumTrees)\n",
    "print(\"Max Depth:\", best_model.getOrDefault(\"maxDepth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy: 0.8204373193740117\n",
    "# Best model hyperparameters:\n",
    "# Num Trees: 10\n",
    "# Max Depth: 15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
