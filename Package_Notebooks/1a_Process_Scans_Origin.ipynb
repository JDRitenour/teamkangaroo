{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d0405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29362/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8329446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31758244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 00:21:06 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.20 instead (on interface wlp61s0)\n",
      "24/06/23 00:21:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 00:21:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExampleApp\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "# Set the log level to ERROR to suppress WARN messages\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c564fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+------------+--------------------+------------+--------------------+\n",
      "|MailPieceID|TrackingEventID|EventZIPCode|   PTSEventTimestamp|PTSEventCode|        PTSEventDesc|\n",
      "+-----------+---------------+------------+--------------------+------------+--------------------+\n",
      "|88266058993|        1000032|       21904|1/19/2024 12:26:3...|          VC|PACKAGE RESEARCH ...|\n",
      "|88251887847|        1000011|       24501|1/20/2024 09:04:1...|          VC|PACKAGE RESEARCH ...|\n",
      "|88310099929|        1000028|       33605|1/20/2024 10:30:2...|          LX| LOOP MAIL EXCEPTION|\n",
      "|88172867268|        1000007|       42104|1/9/2024 18:03:12...|          TM| SHIPMENT ACCEPTANCE|\n",
      "|88173640884|        1000044|       80209|1/16/2024 15:02:3...|          VC|PACKAGE RESEARCH ...|\n",
      "|88371164514|        1000025|       38004|1/24/2024 06:50:1...|          VC|PACKAGE RESEARCH ...|\n",
      "|88247057520|        1000036|       45215|1/17/2024 07:35:0...|          VC|PACKAGE RESEARCH ...|\n",
      "|88275236075|        1000043|       73105|1/24/2024 20:53:1...|          VC|PACKAGE RESEARCH ...|\n",
      "|88129846945|        1000011|       84157|1/9/2024 08:19:57...|          VC|PACKAGE RESEARCH ...|\n",
      "|88254778554|        1000031|       55105|1/18/2024 09:16:2...|          VC|PACKAGE RESEARCH ...|\n",
      "|88224466135|        1000006|       37115|1/11/2024 16:58:0...|          TM| SHIPMENT ACCEPTANCE|\n",
      "|88308201917|        1000007|       38002|1/16/2024 12:17:4...|          TM| SHIPMENT ACCEPTANCE|\n",
      "|88144441120|        1000069|       03102|1/23/2024 21:08:3...|          VC|PACKAGE RESEARCH ...|\n",
      "|88207871975|        1000003|       37230|1/17/2024 15:19:2...|          TM| SHIPMENT ACCEPTANCE|\n",
      "|88175356297|        1000006|       37027|1/10/2024 08:26:3...|          TM| SHIPMENT ACCEPTANCE|\n",
      "|88247627874|        1000006|       38829|1/12/2024 11:15:4...|          TM| SHIPMENT ACCEPTANCE|\n",
      "|88178456998|        1000016|       90014|1/10/2024 18:33:3...|          VC|PACKAGE RESEARCH ...|\n",
      "|88181375466|        1000041|       63133|1/16/2024 16:58:0...|          VC|PACKAGE RESEARCH ...|\n",
      "|88314477452|        1000007|       38801|1/19/2024 15:49:2...|          TM| SHIPMENT ACCEPTANCE|\n",
      "|88247947215|        1000006|       37090|1/12/2024 11:37:1...|          TM| SHIPMENT ACCEPTANCE|\n",
      "+-----------+---------------+------------+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/home/pk/DAEN690/package_data/PKG_Origin_PcScanLevel.txt\", header=True, sep=\"|\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc634825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MailPieceID: string (nullable = true)\n",
      " |-- TrackingEventID: string (nullable = true)\n",
      " |-- EventZIPCode: string (nullable = true)\n",
      " |-- PTSEventTimestamp: string (nullable = true)\n",
      " |-- PTSEventCode: string (nullable = true)\n",
      " |-- PTSEventDesc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af5bd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MailPieceID: string (nullable = true)\n",
      " |-- TrackingEventID: string (nullable = true)\n",
      " |-- EventZIPCode: string (nullable = true)\n",
      " |-- PTSEventTimestamp: timestamp (nullable = true)\n",
      " |-- PTSEventCode: string (nullable = true)\n",
      " |-- PTSEventDesc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "date_time_fields = [\"PTSEventTimestamp\"]\n",
    "df = df.withColumn(\"PTSEventTimestamp\", to_timestamp(\"PTSEventTimestamp\", \"M/d/yyyy HH:mm:ss\"))    \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc80321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+-----------+------------------+--------------------+------------------+\n",
      "|MailPieceID|  min_scan_datetime|  max_scan_datetime|total_scans|Distinct_zip_scans|Distinct_event_scans|time_delta_minutes|\n",
      "+-----------+-------------------+-------------------+-----------+------------------+--------------------+------------------+\n",
      "|88143492237|2024-01-08 16:08:32|2024-01-12 15:55:13|          7|                 3|                   7|              5760|\n",
      "|88145979427|2024-01-08 09:35:29|2024-01-12 10:58:00|          7|                 3|                   7|              5760|\n",
      "|88179517768|2024-01-10 04:45:56|2024-01-13 15:33:20|          3|                 3|                   3|              4320|\n",
      "|88071008889|2024-01-04 20:07:34|2024-01-10 16:46:30|          6|                 4|                   6|              8640|\n",
      "|88151819148|2024-01-08 13:09:54|2024-01-11 11:08:00|          7|                 5|                   6|              4320|\n",
      "|87797523638|2023-12-21 17:33:12|2024-01-12 13:53:00|          7|                 3|                   6|             31680|\n",
      "|88032177135|2024-01-03 10:19:00|2024-01-10 11:45:33|          5|                 3|                   4|             10080|\n",
      "|88316693457|2024-01-16 14:17:22|2024-01-23 10:16:37|          6|                 3|                   5|             10080|\n",
      "|88365328847|2024-01-18 06:20:55|2024-01-29 11:31:01|          7|                 4|                   5|             15840|\n",
      "|88155777703|2024-01-09 01:02:00|2024-01-12 13:44:52|          6|                 2|                   6|              4320|\n",
      "|88198772233|2024-01-10 11:48:13|2024-01-15 10:04:24|          7|                 2|                   7|              7200|\n",
      "|88210462418|2024-01-11 07:56:05|2024-01-12 12:43:54|          6|                 3|                   6|              1440|\n",
      "|88260553905|2024-01-12 00:00:00|2024-01-20 13:08:36|          2|                 2|                   2|             11520|\n",
      "|88184768115|2024-01-10 07:35:49|2024-01-11 19:14:42|          7|                 2|                   7|              1440|\n",
      "|88387586046|2024-01-19 12:26:56|2024-01-23 14:07:09|          6|                 3|                   6|              5760|\n",
      "|88185626396|2024-01-10 09:00:00|2024-01-12 09:17:58|          7|                 3|                   7|              2880|\n",
      "|88338131776|2024-01-17 14:34:09|2024-01-20 10:07:57|          6|                 3|                   6|              4320|\n",
      "|88156807039|2024-01-08 00:00:01|2024-01-11 10:06:46|          6|                 2|                   6|              4320|\n",
      "|88247437765|2024-01-12 14:01:50|2024-01-27 17:37:28|          6|                 3|                   6|             21600|\n",
      "|88172352638|2024-01-08 06:42:57|2024-01-18 14:10:05|          6|                 3|                   6|             14400|\n",
      "+-----------+-------------------+-------------------+-----------+------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"scans\")\n",
    "\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        MailPieceID,\n",
    "        MIN(PTSEventTimestamp) AS min_scan_datetime,\n",
    "        MAX(PTSEventTimestamp) AS max_scan_datetime,\n",
    "        COUNT(*) AS total_scans,\n",
    "        COUNT (DISTINCT EventZIPCode) AS Distinct_zip_scans,\n",
    "        count(distinct PTSEventDesc) as Distinct_event_scans,\n",
    "        (DATEDIFF(MAX(PTSEventTimestamp), MIN(PTSEventTimestamp)) * 24 * 60) AS time_delta_minutes\n",
    "    FROM\n",
    "        scans\n",
    "    GROUP BY\n",
    "        MailPieceID\n",
    "  \n",
    "\"\"\")\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b4ce8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MailPieceID: string (nullable = true)\n",
      " |-- min_scan_datetime: timestamp (nullable = true)\n",
      " |-- max_scan_datetime: timestamp (nullable = true)\n",
      " |-- total_scans: long (nullable = false)\n",
      " |-- Distinct_zip_scans: long (nullable = false)\n",
      " |-- Distinct_event_scans: long (nullable = false)\n",
      " |-- time_delta_minutes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87271d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 12:04:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:04:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:04:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:04:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:04:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:04:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:04:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:04:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:05:17 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:05:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/06/20 12:05:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the result as a Parquet file\n",
    "result_df.write.mode(\"overwrite\").parquet(\"/home/pk/DAEN690/mail_data/Scan Output/result.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3f6e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc9c20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
