{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b030dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15124/912229180.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04dbed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bae0dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/12 10:19:09 WARN Utils: Your hostname, debian resolves to a loopback address: 127.0.1.1; using 192.168.1.20 instead (on interface wlp61s0)\n",
      "24/07/12 10:19:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/12 10:19:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/07/12 10:19:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with the legacy time parser policy and other configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"USPS_scans\") \\\n",
    "    .config(\"spark.executor.memory\", \"8G\") \\\n",
    "    .config(\"spark.driver.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4G\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "556111a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+------------+--------------------+------------+--------------------+\n",
      "|MailPieceID|TrackingEventID|EventZIPCode|   PTSEventTimestamp|PTSEventCode|        PTSEventDesc|\n",
      "+-----------+---------------+------------+--------------------+------------+--------------------+\n",
      "|88266058993|        1000032|       21904|1/19/2024 12:26:3...|          VC|PACKAGE RESEARCH ...|\n",
      "|88251887847|        1000011|       24501|1/20/2024 09:04:1...|          VC|PACKAGE RESEARCH ...|\n",
      "|88310099929|        1000028|       33605|1/20/2024 10:30:2...|          LX| LOOP MAIL EXCEPTION|\n",
      "+-----------+---------------+------------+--------------------+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read csv file with defined schema into Spark DataFrame, and use \",\" delimiter\n",
    "# Read all CSV files into a DataFrame\n",
    "df = spark.read.csv(\"/home/pk/DAEN690/package_data/PKG_Origin_PcScanLevel.txt\", header=True, sep=\"|\")\n",
    "\n",
    "# Show the DataFrame to verify the changes\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff160d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MailPieceID: string (nullable = true)\n",
      " |-- TrackingEventID: string (nullable = true)\n",
      " |-- EventZIPCode: string (nullable = true)\n",
      " |-- PTSEventTimestamp: timestamp (nullable = true)\n",
      " |-- PTSEventCode: string (nullable = true)\n",
      " |-- PTSEventDesc: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert date fields to datetime format\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "date_time_fields = [\"PTSEventTimestamp\"]\n",
    "\n",
    "# Convert the PTSEventTimestamp column to datetime\n",
    "\n",
    "df = df.withColumn(\"PTSEventTimestamp\", to_timestamp(\"PTSEventTimestamp\", \"M/d/yyyy HH:mm:ss\"))\n",
    "    \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191f5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Extract numbers and convert to integer\n",
    "df = df.withColumn(\"PTSEventCode_Numbers\", regexp_extract(\"PTSEventCode\", r'\\b\\d{1,2}\\b', 0).cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244800db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+------------+-------------------+------------+--------------------+--------------------+\n",
      "|MailPieceID|TrackingEventID|EventZIPCode|  PTSEventTimestamp|PTSEventCode|        PTSEventDesc|PTSEventCode_Numbers|\n",
      "+-----------+---------------+------------+-------------------+------------+--------------------+--------------------+\n",
      "|88266058993|        1000032|       21904|2024-01-19 12:26:35|          VC|PACKAGE RESEARCH ...|                NULL|\n",
      "|88251887847|        1000011|       24501|2024-01-20 09:04:13|          VC|PACKAGE RESEARCH ...|                NULL|\n",
      "|88310099929|        1000028|       33605|2024-01-20 10:30:27|          LX| LOOP MAIL EXCEPTION|                NULL|\n",
      "+-----------+---------------+------------+-------------------+------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d78e30b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:===================================================>    (24 + 2) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|43524579|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:=====================================================>  (25 + 1) / 26]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"scans\")\n",
    "\n",
    "# Run the SQL query to calculate time delta and count scans\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    count(*) \n",
    "    from\n",
    "    scans\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec30e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd9f31e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------+\n",
      "|count(DISTINCT PTSEventCode)|count(DISTINCT PTSEventCode_Numbers)|\n",
      "+----------------------------+------------------------------------+\n",
      "|                         132|                                  69|\n",
      "+----------------------------+------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"scans\")\n",
    "\n",
    "# Run the SQL query to calculate time delta and count scans\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    count(distinct PTSEventCode),\n",
    "    count(distinct PTSEventCode_Numbers)\n",
    "    FROM\n",
    "    scans\n",
    "    --order by PTSEventCode_Numbers\n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bde181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+\n",
      "|PTSEventCode_Numbers|PTSEventCode|        PTSEventDesc|\n",
      "+--------------------+------------+--------------------+\n",
      "|                   1|          01|           DELIVERED|\n",
      "|                   2|          02|         NOTICE LEFT|\n",
      "|                   3|          03|    ACCEPT OR PICKUP|\n",
      "|                   4|          04|             REFUSED|\n",
      "|                   5|          05|UNABLE TO DELIVER...|\n",
      "|                   6|          06|           FORWARDED|\n",
      "|                   7|          07|     ARRIVAL AT UNIT|\n",
      "|                   8|          08|             MISSENT|\n",
      "|                   9|          09|    RETURN TO SENDER|\n",
      "|                  10|          10|PROCESSED THROUGH...|\n",
      "|                  11|          11|SEIZED BY LAW ENF...|\n",
      "|                  12|          12|      VISIBLE DAMAGE|\n",
      "|                  13|          13|    AUTHORIZED AGENT|\n",
      "|                  14|          14|AVAILABLE FOR PICKUP|\n",
      "|                  15|          15|         MIS-SHIPPED|\n",
      "|                  16|          16|AVAILABLE FOR RET...|\n",
      "|                  17|          17|TENDERED TO RETUR...|\n",
      "|                  18|          18|     RANDOM SAMPLING|\n",
      "|                  21|          21|      NO SUCH NUMBER|\n",
      "|                  22|          22|INSUFFICIENT ADDRESS|\n",
      "+--------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"scans\")\n",
    "\n",
    "# Run the SQL query to calculate time delta and count scans\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    distinct PTSEventCode_Numbers,\n",
    "    PTSEventCode,\n",
    "    PTSEventDesc\n",
    "    FROM\n",
    "    scans\n",
    "    where PTSEventCode_Numbers is not NULL \n",
    "    order by PTSEventCode,PTSEventCode_Numbers\n",
    "\"\"\")\n",
    "\n",
    "result_df.toPandas().to_csv('PTSEventDesc.csv')\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438be375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = result_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d61ffe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   49432|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"scans\")\n",
    "\n",
    "# Run the SQL query to calculate time delta and count scans\n",
    "result_df = spark.sql(\"\"\"\n",
    "    with temp as (\n",
    "        SELECT\n",
    "        MailPieceID,\n",
    "        PTSEventTimestamp,\n",
    "        PTSEventTimestamp,\n",
    "        EventZIPCode,\n",
    "        PTSEventDesc       \n",
    "        \n",
    "    FROM scans\n",
    "    \n",
    "    where PTSEventCode_Numbers is not NULL \n",
    "    and PTSEventCode_Numbers in (7,10)\n",
    "    \n",
    "    and trim(eventzipcode) in ('37013','37027','37072','37076','37115','37138','37201','37203','37204','37205',\n",
    "                         '37206','37207','37208','37209','37210','37211','37212','37214','37215','37216',\n",
    "                         '37217','37218','37219','37220','37221','37027','37064','37067','37069','37135',\n",
    "                         '37014','37046','37062','37179','37174','37122','37121','37087','37090','37184',\n",
    "                         '37138','37075','37075','37066','37148','37070','37072','37048','37188','37086',\n",
    "                         '37167','37127','37218','37130')\n",
    "        )\n",
    "              \n",
    "        SELECT\n",
    "        count(*)\n",
    "        \n",
    "    FROM temp\n",
    "    \n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "687df80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"scans\")\n",
    "\n",
    "# Run the SQL query to calculate time delta and count scans\n",
    "result_df = spark.sql(\"\"\"\n",
    "    with temp as (\n",
    "        SELECT\n",
    "        MailPieceID,\n",
    "        PTSEventTimestamp,\n",
    "        PTSEventTimestamp,\n",
    "        EventZIPCode,\n",
    "        PTSEventDesc       \n",
    "        \n",
    "    FROM scans\n",
    "    \n",
    "    where PTSEventCode_Numbers is not NULL \n",
    "    and PTSEventCode_Numbers in (7,10)\n",
    "    \n",
    "    and trim(eventzipcode) in ('37013','37027','37072','37076','37115','37138','37201','37203','37204','37205',\n",
    "                         '37206','37207','37208','37209','37210','37211','37212','37214','37215','37216',\n",
    "                         '37217','37218','37219','37220','37221','37027','37064','37067','37069','37135',\n",
    "                         '37014','37046','37062','37179','37174','37122','37121','37087','37090','37184',\n",
    "                         '37138','37075','37075','37066','37148','37070','37072','37048','37188','37086',\n",
    "                         '37167','37127','37218','37130')\n",
    "        )\n",
    "              \n",
    "        SELECT\n",
    "        MailPieceID,\n",
    "        MIN(PTSEventTimestamp) AS min_scan_datetime,\n",
    "        MAX(PTSEventTimestamp) AS max_scan_datetime,\n",
    "        COUNT (DISTINCT EventZIPCode) AS Distinct_zip_scans,\n",
    "        count(distinct PTSEventDesc) as Distinct_event_scans,\n",
    "        (DATEDIFF(MAX(PTSEventTimestamp), MIN(PTSEventTimestamp)) * 24 * 60) AS time_delta_minutes\n",
    "        \n",
    "    FROM temp\n",
    "    group by MailPieceID\n",
    "    order by Distinct_zip_scans desc\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "# Show the result\n",
    "# result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f8398f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44476"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af9c928a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the result as a Parquet file\n",
    "result_df.write.mode(\"overwrite\").parquet(\"/home/pk/DAEN690/Scans_processed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18a33c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817ed59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
